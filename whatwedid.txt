Team: 9
1- Gudjon Magnusson	gudjon@terpmail.umd.edu
2- Irina Yakubinskaya	irinya@terpmail.umd.edu
3- Matthew Goldberg mdgold@cs.umd.edu

Features that were used to improve performance of Trasition-based dependency parser:
The first change we made was running using the larger training set, en.tr, instead of the smaller en.tr100 set. The reasoning behind it was that the bigger training set provides more samples for training. This change resulted in a modest improvement in performance, approximately 10 percentage points for the dev set over our baseline for the previous part (which was ~35% attachment accuracy).
This change seemed a natural one to trry due to the obvious benefits of attempting to train with more data, to have a larger set of English sentences and dependencies to learn from.

In addition to using the larger training set, we implemented two additional features, based on checking the word at the head of the buffer and the coarse POS of the head of the buffer. (Note: our implementation of arc-standard for the previous part used the first formulation of arc-standard presented in the lecture slides, which manipulates the two top items in the stack when creating arcs. Our features for the transparser.py code therefore used the pair from the top of the stack instead of the top of stack + head of buffer.) These two simple features improved performance signficantly from the previous baseline, reaching ~63% attachment accuracy on the dev set with this change alone.
Making the changes of additional features based on the top of the buffer (word and POS) we thought was worth trying due to the intuition that, although our arc-standard implementation immediately works with only entries on the stack, the head of the buffer has data relevant for iterations of the transition algorithm that will soon follow. Therefore, having the perceptron learn features using this identity appears to be relevant for scoring, and able to help the oracle make better decisions.

Combining the above changes resulted in improvement above the goal of 70% on the dev set, yielding roughly 72% performance on average.

Finally, we additionally added features based on the pair of words where one comes from the head of the stack and the other from the head of the buffer, as well as a feature for the pair of POS for these words. Our intuition was that adding this combination would carry additional information compared to the features of stack head and buffer head (or their POS) individually, and would therefore allow the perceptron to better distinguish between transition states. Adding these features further improved performance for the dev set, although by the small amount of ~2 percentage points.
